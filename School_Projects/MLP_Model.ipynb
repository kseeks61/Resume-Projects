{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c88b104a-f9e8-4465-a266-187ab03e3b7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting qiskit\n",
      "  Downloading qiskit-2.2.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (12 kB)\n",
      "Collecting qutip\n",
      "  Downloading qutip-5.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.10.0)\n",
      "Collecting torch\n",
      "  Downloading torch-2.9.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.7.2)\n",
      "Collecting einops\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting rustworkx>=0.15.0 (from qiskit)\n",
      "  Downloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: scipy>=1.5 in /opt/conda/lib/python3.10/site-packages (from qiskit) (1.15.3)\n",
      "Collecting dill>=0.3 (from qiskit)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting stevedore>=3.0.0 (from qiskit)\n",
      "  Downloading stevedore-5.5.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from qiskit) (4.15.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from qutip) (25.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.20.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/conda/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/conda/lib/python3.10/site-packages (from torch) (2025.9.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch)\n",
      "  Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.5.0 (from torch)\n",
      "  Downloading triton-3.5.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (3.0.3)\n",
      "Downloading qiskit-2.2.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading qutip-5.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.0/31.0 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.9.0-cp310-cp310-manylinux_2_28_x86_64.whl (899.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.8/899.8 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m  \u001b[33m0:00:12\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m  \u001b[33m0:00:08\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m107.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m  \u001b[33m0:00:10\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m109.0 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.5.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.3/170.3 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading stevedore-5.5.0-py3-none-any.whl (49 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m137.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, mpmath, triton, sympy, stevedore, rustworkx, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, einops, dill, qutip, qiskit, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/25\u001b[0m [torch]m24/25\u001b[0m [torch]-cusolver-cu12]2]2]\n",
      "\u001b[1A\u001b[2KSuccessfully installed dill-0.4.0 einops-0.8.1 mpmath-1.3.0 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 qiskit-2.2.3 qutip-5.2.1 rustworkx-0.17.1 stevedore-5.5.0 sympy-1.14.0 torch-2.9.0 triton-3.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install qiskit qutip numpy matplotlib torch tqdm scikit-learn einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84649a15-1625-419b-bce4-255bfa40d3e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum object: dims=[[2], [2]], shape=(2, 2), type='oper', dtype=Dense, isherm=True\n",
      "Qobj data =\n",
      "[[ 0.23863593+0.j         -0.32723432-0.12202914j]\n",
      " [-0.32723432+0.12202914j  0.76136407+0.j        ]]\n"
     ]
    }
   ],
   "source": [
    "from qutip import basis, ket2dm, Qobj\n",
    "import numpy as np\n",
    "\n",
    "def mixed_state_generation(n_samples=10000):\n",
    "    mixed_states = []\n",
    "    for _ in range(n_samples):\n",
    "        # Random angles on Bloch sphere\n",
    "        theta = np.arccos(2*np.random.rand() - 1)  # polar angle [0, pi]\n",
    "        phi = 2*np.pi*np.random.rand()             # azimuthal angle [0, 2pi]\n",
    "\n",
    "        # Construct state |ψ> = cos(theta/2)|0> + exp(i*phi)*sin(theta/2)|1>\n",
    "        psi = np.cos(theta/2)*basis(2,0) + np.exp(1j*phi)*np.sin(theta/2)*basis(2,1)\n",
    "        pure_dm = ket2dm(psi)\n",
    "        \n",
    "        # Random mixing with |1> to get mixed state\n",
    "        p = np.random.rand()\n",
    "        rho_mixed = p * pure_dm + (1-p) * ket2dm(basis(2,1))\n",
    "        mixed_states.append(rho_mixed)\n",
    "            \n",
    "    return mixed_states\n",
    "\n",
    "mixed_state_data = mixed_state_generation()\n",
    "print(mixed_state_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95205a4f-e029-4974-b43c-a35db220ad81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Quantum object: dims=[[2], [2]], shape=(2, 2), type='oper', dtype=Dense, isherm=True\n",
      "Qobj data =\n",
      "[[0.61931797+0.j         0.23138961+0.08628763j]\n",
      " [0.23138961-0.08628763j 0.38068203+0.j        ]], Quantum object: dims=[[2], [2]], shape=(2, 2), type='oper', dtype=Dense, isherm=True\n",
      "Qobj data =\n",
      "[[ 0.61931797+0.j         -0.23138961-0.08628763j]\n",
      " [-0.23138961+0.08628763j  0.38068203+0.j        ]])\n"
     ]
    }
   ],
   "source": [
    "from qutip import Qobj, ket2dm, tensor, ptrace\n",
    "import numpy as np # Import numpy\n",
    "\n",
    "def generate_clones(mixed_state_data):\n",
    "  cloned_pairs = []\n",
    "  U_clone = Qobj([[1,0,0,0],\n",
    "                [0,1/np.sqrt(2),1/np.sqrt(2),0],\n",
    "                [0,1/np.sqrt(2),-1/np.sqrt(2),0],\n",
    "                [0,0,0,1]], dims=[[2, 2], [2, 2]]) # Corrected dimensions\n",
    "  ancilla = ket2dm(basis(2,0))          # Blank qubit |0>\n",
    "  for rho in mixed_state_data:  # mixed_states is your list of 10k arrays\n",
    "    rho_joint = tensor(rho, ancilla)      # Combine system\n",
    "\n",
    "    rho_cloned = U_clone * rho_joint * U_clone.dag()  # apply unitary\n",
    "\n",
    "    clone1 = ptrace(rho_cloned, 0)\n",
    "    clone2 = ptrace(rho_cloned, 1)\n",
    "    cloned_pairs.append((clone1, clone2))\n",
    "  return cloned_pairs\n",
    "\n",
    "generated_clones = generate_clones(mixed_state_data)\n",
    "print(generated_clones[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47968775-5f18-4166-8d66-1d2a73f92832",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3)\n",
      "(10000, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from qutip import sigmax, sigmay, sigmaz, Qobj\n",
    "\n",
    "def qobj_to_bloch(rho):\n",
    "    \"\"\"Convert a single QuTiP Qobj density matrix to a Bloch vector.\"\"\"\n",
    "    x = np.real((rho * sigmax()).tr())\n",
    "    y = np.real((rho * sigmay()).tr())\n",
    "    z = np.real((rho * sigmaz()).tr())\n",
    "    return np.array([x, y, z])\n",
    "\n",
    "def convert_pairs_to_bloch(pair_list):\n",
    "    \"\"\"\n",
    "    Convert a list of (rho1, rho2) pairs to Bloch vectors.\n",
    "\n",
    "    Returns two arrays:\n",
    "      - bloch1: first clones\n",
    "      - bloch2: second clones\n",
    "    \"\"\"\n",
    "    bloch1 = []\n",
    "    bloch2 = []\n",
    "\n",
    "    for rho1, rho2 in pair_list:\n",
    "        bloch1.append(qobj_to_bloch(rho1))\n",
    "        bloch2.append(qobj_to_bloch(rho2))\n",
    "\n",
    "    return np.array(bloch1), np.array(bloch2)\n",
    "\n",
    "# Example usage\n",
    "# pair_list = [(rho1_clone1, rho1_clone2), (rho2_clone1, rho2_clone2), ...]\n",
    "bloch_clone1, bloch_clone2 = convert_pairs_to_bloch(generated_clones)\n",
    "\n",
    "print(bloch_clone1.shape)  # (10000, 3)\n",
    "print(bloch_clone2.shape)  # (10000, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d560ea3c-919a-449f-9133-807b24bbb077",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3)\n"
     ]
    }
   ],
   "source": [
    "def convert_original_to_bloch(original_list):\n",
    "    bloch = []\n",
    "\n",
    "    for rho in original_list:\n",
    "        bloch.append(qobj_to_bloch(rho))\n",
    "\n",
    "\n",
    "    return np.array(bloch)\n",
    "\n",
    "bloch_original = convert_original_to_bloch(mixed_state_data)\n",
    "\n",
    "print(bloch_original.shape)  # (10000, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "276f3fde-46fa-4201-a489-482d7a721985",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "x_train_orig, x_test_orig, x_train_c1, x_test_c1, x_train_c2, x_test_c2 = train_test_split(\n",
    "    bloch_original, bloch_clone1, bloch_clone2, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Stack clones to create inputs and targets\n",
    "X_train = np.vstack([x_train_c1, x_train_c2]).astype(np.float32)\n",
    "y_train = np.vstack([x_train_orig, x_train_orig]).astype(np.float32)\n",
    "\n",
    "X_test = np.vstack([x_test_c1, x_test_c2]).astype(np.float32)\n",
    "y_test = np.vstack([x_test_orig, x_test_orig]).astype(np.float32)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# PyTorch Dataset\n",
    "# -------------------------------\n",
    "class BlochDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = BlochDataset(X_train, y_train)\n",
    "test_dataset = BlochDataset(X_test, y_test)\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fadcbaa-5ab8-49cf-a613-5cc03a7e7e36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 0.108738 | Test Loss: 0.077361\n",
      "Epoch 2/50 | Train Loss: 0.075354 | Test Loss: 0.077330\n",
      "Epoch 3/50 | Train Loss: 0.075184 | Test Loss: 0.077351\n",
      "Epoch 4/50 | Train Loss: 0.075226 | Test Loss: 0.077308\n",
      "Epoch 5/50 | Train Loss: 0.075128 | Test Loss: 0.077378\n",
      "Epoch 6/50 | Train Loss: 0.075150 | Test Loss: 0.077207\n",
      "Epoch 7/50 | Train Loss: 0.075197 | Test Loss: 0.077075\n",
      "Epoch 8/50 | Train Loss: 0.075127 | Test Loss: 0.077141\n",
      "Epoch 9/50 | Train Loss: 0.075179 | Test Loss: 0.077744\n",
      "Epoch 10/50 | Train Loss: 0.075153 | Test Loss: 0.077195\n",
      "Epoch 11/50 | Train Loss: 0.075187 | Test Loss: 0.077046\n",
      "Epoch 12/50 | Train Loss: 0.075117 | Test Loss: 0.077209\n",
      "Epoch 13/50 | Train Loss: 0.075227 | Test Loss: 0.077331\n",
      "Epoch 14/50 | Train Loss: 0.075117 | Test Loss: 0.077234\n",
      "Epoch 15/50 | Train Loss: 0.075087 | Test Loss: 0.077384\n",
      "Epoch 16/50 | Train Loss: 0.075103 | Test Loss: 0.077725\n",
      "Epoch 17/50 | Train Loss: 0.075166 | Test Loss: 0.077251\n",
      "Epoch 18/50 | Train Loss: 0.075124 | Test Loss: 0.077220\n",
      "Epoch 19/50 | Train Loss: 0.075094 | Test Loss: 0.077725\n",
      "Epoch 20/50 | Train Loss: 0.075119 | Test Loss: 0.077283\n",
      "Epoch 21/50 | Train Loss: 0.075093 | Test Loss: 0.077461\n",
      "Epoch 22/50 | Train Loss: 0.075085 | Test Loss: 0.077477\n",
      "Epoch 23/50 | Train Loss: 0.075062 | Test Loss: 0.077273\n",
      "Epoch 24/50 | Train Loss: 0.075096 | Test Loss: 0.077266\n",
      "Epoch 25/50 | Train Loss: 0.075112 | Test Loss: 0.077129\n",
      "Epoch 26/50 | Train Loss: 0.075086 | Test Loss: 0.077267\n",
      "Epoch 27/50 | Train Loss: 0.075053 | Test Loss: 0.077186\n",
      "Epoch 28/50 | Train Loss: 0.075056 | Test Loss: 0.077234\n",
      "Epoch 29/50 | Train Loss: 0.075057 | Test Loss: 0.077268\n",
      "Epoch 30/50 | Train Loss: 0.075074 | Test Loss: 0.077176\n",
      "Epoch 31/50 | Train Loss: 0.075050 | Test Loss: 0.077166\n",
      "Epoch 32/50 | Train Loss: 0.075066 | Test Loss: 0.077389\n",
      "Epoch 33/50 | Train Loss: 0.075059 | Test Loss: 0.077415\n",
      "Epoch 34/50 | Train Loss: 0.075096 | Test Loss: 0.077251\n",
      "Epoch 35/50 | Train Loss: 0.075063 | Test Loss: 0.077171\n",
      "Epoch 36/50 | Train Loss: 0.075044 | Test Loss: 0.077523\n",
      "Epoch 37/50 | Train Loss: 0.075062 | Test Loss: 0.077399\n",
      "Epoch 38/50 | Train Loss: 0.075010 | Test Loss: 0.077210\n",
      "Epoch 39/50 | Train Loss: 0.075036 | Test Loss: 0.077292\n",
      "Epoch 40/50 | Train Loss: 0.075057 | Test Loss: 0.077136\n",
      "Epoch 41/50 | Train Loss: 0.075040 | Test Loss: 0.077296\n",
      "Epoch 42/50 | Train Loss: 0.075024 | Test Loss: 0.077235\n",
      "Epoch 43/50 | Train Loss: 0.075021 | Test Loss: 0.077314\n",
      "Epoch 44/50 | Train Loss: 0.075058 | Test Loss: 0.077295\n",
      "Epoch 45/50 | Train Loss: 0.075019 | Test Loss: 0.077165\n",
      "Epoch 46/50 | Train Loss: 0.075012 | Test Loss: 0.077446\n",
      "Epoch 47/50 | Train Loss: 0.075041 | Test Loss: 0.077217\n",
      "Epoch 48/50 | Train Loss: 0.075010 | Test Loss: 0.077163\n",
      "Epoch 49/50 | Train Loss: 0.075027 | Test Loss: 0.077247\n",
      "Epoch 50/50 | Train Loss: 0.075016 | Test Loss: 0.077460\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BlochDenoiser(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(3, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = BlochDenoiser()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "n_epochs = 50\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * X_batch.size(0)\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            output = model(X_batch)\n",
    "            test_loss += criterion(output, y_batch).item() * X_batch.size(0)\n",
    "    avg_test_loss = test_loss / len(test_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} | Train Loss: {avg_loss:.6f} | Test Loss: {avg_test_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a044d9a3-e719-4368-8018-2f49bd136ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Make sure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Convert your test data to torch tensors if not already\n",
    "X_test_tensor = torch.from_numpy(X_test)  # shape: (n_samples*2, 3)\n",
    "y_test_tensor = torch.from_numpy(y_test)  # shape: (n_samples*2, 3)\n",
    "\n",
    "# Run the model on test data\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)\n",
    "\n",
    "# Convert predictions back to NumPy\n",
    "y_pred_np = y_pred.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c537ebb-64c8-4fc7-8a50-ed861198c9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average pre trained fidelity: 0.709880\n",
      "Average post trained fidelity: 0.924395\n",
      "True: [-0.53195876  0.40049288  0.03133399], Predicted: [ 0.02680653 -0.02383776  0.02983391], Fidelity: 0.860866\n",
      "True: [-0.22185768  0.7300604  -0.49764797], Predicted: [ 0.0279863  -0.01694597 -0.49858037], Fidelity: 0.793418\n",
      "True: [-0.24586426 -0.7821866   0.34952572], Predicted: [ 0.01299435 -0.01587716  0.3474653 ], Fidelity: 0.777858\n",
      "True: [ 0.3833104  -0.14213307 -0.39633077], Predicted: [ 0.02699019 -0.00095847 -0.39650863], Fidelity: 0.960993\n",
      "True: [0.6734333  0.28048235 0.5989049 ], Predicted: [-0.03570561  0.01990424  0.5775033 ], Fidelity: 0.798381\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bloch_fidelity_batch(rho_array, sigma_array):\n",
    "    \"\"\"\n",
    "    Compute fidelities between two arrays of Bloch vectors.\n",
    "\n",
    "    Parameters:\n",
    "        rho_array: np.array of shape (n_samples, 3) - true vectors\n",
    "        sigma_array: np.array of shape (n_samples, 3) - predicted vectors\n",
    "\n",
    "    Returns:\n",
    "        fidelities: np.array of shape (n_samples,)\n",
    "    \"\"\"\n",
    "    r_dot = np.sum(rho_array * sigma_array, axis=1)           # dot product for each sample\n",
    "    r_norm = np.linalg.norm(rho_array, axis=1)               # magnitude of true vectors\n",
    "    s_norm = np.linalg.norm(sigma_array, axis=1)             # magnitude of predicted vectors\n",
    "    fidelities = 0.5 * (1 + r_dot + np.sqrt(1 - r_norm**2) * np.sqrt(1 - s_norm**2))\n",
    "    return fidelities\n",
    "\n",
    "# Example usage:\n",
    "fidelities_clone_orig = bloch_fidelity_batch(y_test, X_test)\n",
    "fidelities_pred_true = bloch_fidelity_batch(y_test, y_pred_np)\n",
    "\n",
    "# Average fidelity over all test samples\n",
    "print(f\"Average pre trained fidelity: {np.mean(fidelities_clone_orig):.6f}\")\n",
    "\n",
    "# Average fidelity over all test samples\n",
    "print(f\"Average post trained fidelity: {np.mean(fidelities_pred_true):.6f}\")\n",
    "\n",
    "# Optional: inspect first few\n",
    "for i in range(5):\n",
    "    print(f\"True: {y_test[i]}, Predicted: {y_pred_np[i]}, Fidelity: {fidelities_pred_true[i]:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe48bb78-8949-4794-ab1d-6d61a5dab6ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"MLP.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24bfd02f-6c37-4d03-bd64-fdb83b9f8e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "# Initialize a client\n",
    "client = storage.Client()\n",
    "\n",
    "# Reference your bucket\n",
    "bucket_name = \"instr-cs795-fall25-hqin-1-kseek001\"   # example: \"kris-models\"\n",
    "bucket = client.bucket(bucket_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ece79e03-62a2-4ed5-bf47-7063d1e502fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://model.pth [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  4.2 KiB/  4.2 KiB]                                                \n",
      "Operation completed over 1 objects/4.2 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp model.pth gs://instr-cs795-fall25-hqin-1-kseek001/torch_models/model.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971f51b2-d070-479f-85b0-c2f2861203a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m134",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m134"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
