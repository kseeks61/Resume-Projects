# -*- coding: utf-8 -*-
"""CS722_Proj.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nd5drR7PIuId8V8TaCGp6gCsl91WDWDO
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import math
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from imblearn.over_sampling import SMOTE
from sklearn.metrics import roc_curve, auc, recall_score, precision_score, accuracy_score
from google.colab import drive
drive.mount('/content/drive')

df_author = pd.read_csv('/content/drive/MyDrive/allwine.csv')

X_author = df_author.drop('quality', axis=1)
y_author = df_author['quality']

# Standardize the features using the same scaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_author)


X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_author, test_size=0.2, random_state=42)

X_train_author = np.array(X_train)
y_train_author = np.array(y_train)
X_test_author = np.array(X_test)
y_test_author = np.array(y_test)

def weightInitialization(n_features,n_layers, seed=None):
    if seed is not None:
        np.random.seed(seed)  # Initialize the random seed for reproducibility
    weights = np.random.rand(abs(np.power(2,n_layers)-1), n_features) #np.zeros((1,n_features))  # 2D array for weights
    bias = np.reshape(np.zeros(abs(np.power(2,n_layers)-1)), (abs(np.power(2,n_layers)-1), 1)) #np.random.rand(1, n_layers)
    return weights, bias

def log_regression(X, weights, bias, j):
    z = np.dot(weights[int(j - 1), :], X.T) + bias[int(j - 1), :] #Stores z as the sigmoid function
    z = np.clip(z, -500, 500)  # Prevents over flow and gradient explosion
    return 1 / (1 + np.exp(-z))

def path_probability(X, weights, bias, j, num_layers):
    #If it is the first node, the path probability is 1 since there is a 100% chance of reaching the first node
    if j == 1:
        return 1
    #Calculates the path probability for all other nodes (What is the probability of that node being reached?)
    #Acts as an extra weight to emphasize certain nodes are more likely to be activated than others
    return path_probability(X, weights, bias, np.floor((j)/2), num_layers-1) * (log_regression(X, weights, bias, np.floor((j)/2))**((j+1)%2)) * ((1 - log_regression(X, weights, bias, np.floor((j)/2)))**((j)%2))

def recursive_prob(X, y, weights, bias, num_layers, node=1):
    #If there is only one layer, the recursive probability is just a normal log function
    if num_layers == 1:
        return (log_regression(X, weights, bias, node) ** y) * ((1 - log_regression(X, weights, bias, node)) ** (1 - y))
    if 2 * node >= 2 ** (num_layers - 1):  # Final layer (leaf nodes)
        return (log_regression(X, weights, bias, node) ** y) * ((1 - log_regression(X, weights, bias, node)) ** (1 - y))
    elif 2 * node >= 2 ** (num_layers - 2) and num_layers > 2:  # Second-to-last layer (equation 10)
        return log_regression(X, weights, bias, node) * ((log_regression(X, weights, bias, 2 * node) ** y) * ((1 - log_regression(X, weights, bias, 2 * node)) ** (1 - y)) -(log_regression(X, weights, bias, 2 * node + 1) ** y) * ((1 - log_regression(X, weights, bias, 2 * node + 1)) ** (1 - y))) + (log_regression(X, weights, bias, 2 * node + 1) ** y) * ((1 - log_regression(X, weights, bias, 2 * node + 1)) ** (1 - y))
    else:  # Other nodes (equation 9)
        return log_regression(X, weights, bias, node) * (recursive_prob(X, y, weights, bias, num_layers, 2 * node) -recursive_prob(X, y, weights, bias, num_layers, 2 * node + 1)) + recursive_prob(X, y, weights, bias, num_layers, 2 * node + 1)

def node_prob(X, y, weights, bias, node):
    #Just gives the probability output for the current node
    return (log_regression(X, weights, bias, node) ** y) * ((1 - log_regression(X, weights, bias, node)) ** (1 - y))

def update_gradient(X, y, weights, bias, num_layers, node, j, count):
    #This function is used to update the gradient for other nodes where:
      #The node is less than 2 to the power of the number of layers minus
      #And the number of layers are greater than 2
    #The rule is applied (n-[log2 j+2]) times as shown by the stop count value
    stop_count = num_layers - np.floor(np.log2(j) + 2)

    if count == stop_count: #If at the final iteration, output the final gradient update
        result = log_regression(X, weights, bias, node) * ((log_regression(X, weights, bias, 2 * node) ** y) * ((1 - log_regression(X, weights, bias, 2 * node)) ** (1 - y)) -(log_regression(X, weights, bias, (2 * node) + 1) ** y) * ((1 - log_regression(X, weights, bias, (2 * node) + 1)) ** (1 - y))) + (log_regression(X, weights, bias, (2 * node) + 1) ** y) * ((1 - log_regression(X, weights, bias, (2 * node) + 1)) ** (1 - y))
        return result
    else: #If not at the final iteration, update the update_gradient function until termination rule is met
        count = count + 1
        result = log_regression(X, weights, bias, node) * (update_gradient(X, y, weights, bias, num_layers, 2 * node, j, count) - update_gradient(X, y, weights, bias, num_layers, 2 * node + 1, j, count)) + update_gradient(X, y, weights, bias, num_layers, 2 * node + 1, j, count)
        return result

def calc_gradient(X, y, weights, bias, num_layers, recurse_prob):
    m = X.shape[0]
    num_nodes = weights.shape[0]
    weights_gradient = np.zeros_like(weights) #Making sure that the weight and bias are not set to exactly zero
    bias_gradient = np.zeros_like(bias)
    for j in range(num_nodes): #Looping over all nodes in the ensemble
        if j + 1 >= 2 ** (num_layers - 1):  #Identifies the leaf nodes
            #Calculating the path probability for the current node
            p_path = path_probability(X, weights, bias, j+1, num_layers)
            #Calculating the log regression for the current node
            h_val = log_regression(X, weights, bias, j+1)
            #Calculating the gradient for the current node
            gradient = (p_path * (h_val ** y) * ((1 - h_val) ** (1 - y)) * (y - h_val)) / recurse_prob
            #Reshaping the gradient to ensure it can be applied to weights and bias
            gradient = gradient.reshape(-1, 1)
            #Applying the gradients to the weight and bias for current node
            weights_gradient[j] = -np.mean(np.dot(X.T, gradient), axis=1)
            bias_gradient[j] = -np.mean(gradient)
        elif 2 ** (num_layers - 2) <= j + 1 < 2 ** (num_layers - 1): #Applies gradient descent to immediate parents of leaf nodes
            #Calculating the path probability for the current node
            p_path = path_probability(X, weights, bias, j + 1, num_layers - 1)
            #Calculating the log regressions for the current node and its children nodes
            h_val = log_regression(X, weights, bias, j+1)
            h_val2 = log_regression(X, weights, bias, 2 * (j+1))
            h_val2_plus_1 = log_regression(X, weights, bias, (2 * (j+1) + 1))
            #Calculating the gradient for the current node
            gradient = (p_path * h_val * (1 - h_val) * (((h_val2 ** y) * ((1 - h_val2) ** (1 - y))) - (h_val2_plus_1 ** y) * ((1 - h_val2_plus_1) ** (1 - y)))) / recurse_prob
            #Reshaping the gradient to ensure it can be applied to weights and bias
            gradient = gradient.reshape(-1, 1)
            #Applying the gradients to the weight and bias for current node
            weights_gradient[j] = -np.mean(np.dot(X.T, gradient), axis=1)
            bias_gradient[j] = -np.mean(gradient)
        elif num_layers > 2 and j+1 < 2 ** (num_layers - 2):
            '''This is differnt than the other nodes before. This method is only done if the current node is not a leaf node or a parent
            of an immediate leaf node. The gradient is updated (n-[log2 j+2]) times, using equation (33) in the paper.'''
            ##Calculating the path probability for the current node
            p_path = path_probability(X, weights, bias, j+1, np.floor(np.log2(j+1) + 2) - 1)
            #Calculating the log regression for the current node
            h_val = log_regression(X, weights, bias, j+1)
            #Calculating the gradient for the current node
            gradient = (p_path * h_val * (1 - h_val) * (update_gradient(X, y, weights, bias, num_layers, 2 * (j+1), j, 1) - update_gradient(X, y, weights, bias, num_layers, (2 * (j+1)) + 1, j, 1))) / recurse_prob
            #Reshaping the gradient to ensure it can be applied to weights and bias
            gradient = gradient.reshape(-1, 1)
            #Applying the gradients to the weight and bias for current node
            weights_gradient[j] = -np.mean(np.dot(X.T, gradient), axis=1)
            bias_gradient[j] = -np.mean(gradient)
    return weights_gradient, bias_gradient

def model_optimize(X,y,weights,bias,num_layers=1):
  #Running the training data through the algorithm to optimize weights before making predictions
    m = X.shape[0]
    recurse_prob = recursive_prob(X,y,weights,bias,num_layers)
    recurse_prob[recurse_prob==0] = 0.00000000001 #Replacing a recursive probability of zero with a small value
    cost = (-1/m)*(np.sum(np.log(recurse_prob))) #Calculates the cost function
    derived_weight, derived_bias = calc_gradient(X,y,weights,bias,num_layers,recurse_prob) #Calculating the gradients to reduce the weights by
    #Storing the gradient weights and bias
    gradients = {"dw": derived_weight, "db": derived_bias}

    return gradients, cost

def model_predict(X, y, weights, bias, learning_rate, epochs):
  #Updating the weights and biases based on the gradient calculation, then running through the model for each epoch
    costs = []
    for i in range(epochs):
        #Calculating the gradients and cost for the current epoch
        grads, cost = model_optimize(X, y.T, weights, bias)
        #Unpacks the gradient for weight and bias
        dw = grads["dw"]
        db = grads["db"]
        #Update the wieghts and bias by the gradient multiplied by the learning rate
        weights = weights - (learning_rate * dw)
        bias = bias - (learning_rate * db)
        if i % 100 == 0: #Puts every 100th cost into a list for making a chart later
            costs.append(cost)
    #Stores the final weights and bias for test prediction
    coeff = {"w": weights, "b": bias}
    gradient = {"dw": dw, "db": db}
    return coeff, gradient, costs

def predict(final_pred, m): #Predicting the y values for each example
    #Initializing an array with 1 row and 0 for the number of m rows
    y_pred = np.zeros((1,m))
    #Iterating over all the rows of the final probabilities for each example (Prob the class is 1)
    for i in range(final_pred.shape[1]):
        #Converting the probability value to either 1 or 0 to represent the class and save it to the y_pred array
        if final_pred[0][i] >= 0.5:
            y_pred[0][i] = 1
    return y_pred

num_layers = 1
n_features = X_train_author.shape[1] #Outputs the number of features in X
epochs = 4500
learning_rate = 0.00001

#Initializaing the weights and bias
weights, bias= weightInitialization(n_features,num_layers)
#Getting the weights, bias, and costs after training the model
coeff, gradient, costs = model_predict(X_train_author,y_train_author,weights,bias,learning_rate,epochs)
#Unpacking the weights and bias
weights, bias = coeff["w"], coeff["b"]

#Using the final weights and bias to make the probilities that each example is class 1
final_train_pred = recursive_prob(X_train_author, 1, weights, bias, num_layers) #For the training final probabilities
final_test_pred = recursive_prob(X_test_author, 1, weights, bias, num_layers)  #For the final testing probabilities

#Storing the number of examples for the test and training predictions
m_train = X_train_author.shape[0]
m_test = X_test_author.shape[0]

#Outputting the class labels for each example based on the final probabilities calculated
y_train_pred = predict(final_train_pred.reshape(1, -1), m_train)
y_test_pred = predict(final_test_pred.reshape(1, -1), m_test)

# Evaluation Methods
train_accuracy = accuracy_score(y_train_pred.T, y_train_author)
test_accuracy = accuracy_score(y_test_pred.T, y_test_author)
fpr, tpr, _ = roc_curve(y_test_author, final_test_pred.T)
roc_auc = auc(fpr, tpr)
recall = recall_score(y_test_author, y_test_pred.T)
precision = precision_score(y_test_author, y_test_pred.T)

print(f"Final Cost: {costs[-1]:.4f}")
print(f"Training Accuracy: {train_accuracy:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")
print(f"Test AUC: {roc_auc:.4f}")
print(f"Test Recall: {recall:.4f}")
print(f"Test Precision: {precision:.4f}")

# Plot ROC Curve
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

# Plot Cost Reduction
plt.plot(costs)
plt.ylabel('cost')
plt.xlabel('iterations (per hundreds)')
plt.title('Cost reduction over time')
plt.show()

num_layers = 2
scaler = StandardScaler()
n_features = X_train_author.shape[1]
epochs = 4500
learning_rate = 0.00001
weights, bias= weightInitialization(n_features,num_layers)
coeff, gradient, costs = model_predict(X_train_author,y_train_author,weights,bias,learning_rate,epochs)
weights, bias = coeff["w"], coeff["b"]

final_train_pred = recursive_prob(X_train_author, 1, weights, bias, num_layers)
final_test_pred = recursive_prob(X_test_author, 1, weights, bias, num_layers)


m_train = X_train_author.shape[0]
m_test = X_test_author.shape[0]
y_train_pred = predict(final_train_pred.reshape(1, -1), m_train)
y_test_pred = predict(final_test_pred.reshape(1, -1), m_test)



# Evaluate
train_accuracy = accuracy_score(y_train_pred.T, y_train_author)
test_accuracy = accuracy_score(y_test_pred.T, y_test_author)
fpr, tpr, _ = roc_curve(y_test_author, final_test_pred.T)
roc_auc = auc(fpr, tpr)
recall = recall_score(y_test_author, y_test_pred.T)
precision = precision_score(y_test_author, y_test_pred.T)

print(f"Final Cost: {costs[-1]:.4f}")
print(f"Training Accuracy: {train_accuracy:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")
print(f"Test AUC: {roc_auc:.4f}")
print(f"Test Recall: {recall:.4f}")
print(f"Test Precision: {precision:.4f}")

# Plot ROC Curve
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

# Plot Cost Reduction
plt.plot(costs)
plt.ylabel('cost')
plt.xlabel('iterations (per hundreds)')
plt.title('Cost reduction over time')
plt.show()

num_layers = 3
n_features = X_train_author.shape[1]
epochs = 4500
learning_rate = 0.00001
weights, bias= weightInitialization(n_features,num_layers)
coeff, gradient, costs = model_predict(X_train_author,y_train_author,weights,bias,learning_rate,epochs)
weights, bias = coeff["w"], coeff["b"]

final_train_pred = recursive_prob(X_train_author, 1, weights, bias, num_layers)
final_test_pred = recursive_prob(X_test_author, 1, weights, bias, num_layers)


m_train = X_train_author.shape[0]
m_test = X_test_author.shape[0]
y_train_pred = predict(final_train_pred.reshape(1, -1), m_train)
y_test_pred = predict(final_test_pred.reshape(1, -1), m_test)



# Evaluate
train_accuracy = accuracy_score(y_train_pred.T, y_train_author)
test_accuracy = accuracy_score(y_test_pred.T, y_test_author)
fpr, tpr, _ = roc_curve(y_test_author, final_test_pred.T)
roc_auc = auc(fpr, tpr)
recall = recall_score(y_test_author, y_test_pred.T)
precision = precision_score(y_test_author, y_test_pred.T)

print(f"Final Cost: {costs[-1]:.4f}")
print(f"Training Accuracy: {train_accuracy:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")
print(f"Test AUC: {roc_auc:.4f}")
print(f"Test Recall: {recall:.4f}")
print(f"Test Precision: {precision:.4f}")

    # Plot ROC Curve
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

# Plot Cost Reduction
plt.plot(costs)
plt.ylabel('cost')
plt.xlabel('iterations (per hundreds)')
plt.title('Cost reduction over time')
plt.show()

num_layers = 4
n_features = X_train_author.shape[1]
epochs = 4500
learning_rate = 0.00001
weights, bias= weightInitialization(n_features,num_layers)
coeff, gradient, costs = model_predict(X_train_author,y_train_author,weights,bias,learning_rate,epochs)
weights, bias = coeff["w"], coeff["b"]

final_train_pred = recursive_prob(X_train_author, 1, weights, bias, num_layers)
final_test_pred = recursive_prob(X_test_author, 1, weights, bias, num_layers)


m_train = X_train_author.shape[0]
m_test = X_test_author.shape[0]
y_train_pred = predict(final_train_pred.reshape(1, -1), m_train)
y_test_pred = predict(final_test_pred.reshape(1, -1), m_test)



# Evaluate
train_accuracy = accuracy_score(y_train_pred.T, y_train_author)
test_accuracy = accuracy_score(y_test_pred.T, y_test_author)
fpr, tpr, _ = roc_curve(y_test_author, final_test_pred.T)
roc_auc = auc(fpr, tpr)
recall = recall_score(y_test_author, y_test_pred.T)
precision = precision_score(y_test_author, y_test_pred.T)

print(f"Final Cost: {costs[-1]:.4f}")
print(f"Training Accuracy: {train_accuracy:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")
print(f"Test AUC: {roc_auc:.4f}")
print(f"Test Recall: {recall:.4f}")
print(f"Test Precision: {precision:.4f}")

    # Plot ROC Curve
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

# Plot Cost Reduction
plt.plot(costs)
plt.ylabel('cost')
plt.xlabel('iterations (per hundreds)')
plt.title('Cost reduction over time')
plt.show()

df = pd.read_csv('/content/drive/MyDrive/diabetes.csv')

df.head()

df.describe()

df.info()

df['Outcome'].value_counts().plot.bar()

df.columns

X = df.drop('Outcome', axis=1)
y = df['Outcome']
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

smote = SMOTE(random_state=42)
X_train, y_train = smote.fit_resample(X_train, y_train)
y_train.value_counts().plot.bar()

X_tr_arr = X_train
X_ts_arr = X_test
y_tr_arr = np.array(y_train)
y_ts_arr = np.array(y_test)

print('Input Shape', (X_tr_arr.shape))
print('Output Shape', X_test.shape)

n_features = X_tr_arr.shape[1]
print('Number of Features', n_features)

def weightInitialization(n_features,n_layers, seed=None):
    if seed is not None:
        np.random.seed(seed)  # Initialize the random seed for reproducibility
    w = np.random.rand(abs(np.power(2,n_layers)-1), n_features) #np.zeros((1,n_features))  # 2D array for weights
    b = np.reshape(np.zeros(abs(np.power(2,n_layers)-1)), (abs(np.power(2,n_layers)-1), 1)) #np.random.rand(1, n_layers)
    return w, b
    # w[value-1 node, value-1 feature]

def h(j,w,b,X):
    return 1/(1+np.exp(-(np.dot(w[int(j-1),:],X.T)+b[int(j-1),:])))

def p_path_probability(n_layers,j,w,b,X):
    if j == 1:
        return 1
    return p_path_probability(n_layers-1,np.floor((j)/2),w,b,X) * (h(np.floor((j)/2),w,b,X)**((j+1)%2)) * ((1-h(np.floor((j)/2),w,b,X))**((j)%2))

def P(Y,X,n_layers,w,b,node=1):
    if n_layers==1:
        return (h(node,w,b,X)**Y) * ((1-h(node,w,b,X))**(1-Y))
    if 2*node < 2**(n_layers-1):
        return (h(node,w,b,X) * (P(Y,X,n_layers,w,b,2*node) - P(Y,X,n_layers,w,b,2*node+1))) + P(Y,X,n_layers,w,b,2*node+1)
    elif 2*node >= 2**(n_layers-1):
        return (h(node,w,b,X) * (((h(2*node,w,b,X)**Y) * ((1-h(2*node,w,b,X))**(1-Y))) - ((h(2*node+1,w,b,X)**Y) * ((1-h(2*node+1,w,b,X))**(1-Y)))) )+ ((h(2*node+1,w,b,X)**Y) * ((1-h(2*node+1,w,b,X))**(1-Y)))

def p_k(j,w,b,X,Y):
    return (h(j,w,b,X)**Y) * ((1-h(j,w,b,X))**(1-Y))

def update_gradient(w,b,X,Y,n_layers,node,j,count):
  if count == n_layers-np.floor(np.log2(j)+2):
    return (h(node,w,b,X) * (((h(2*node,w,b,X)**Y) * ((1-h((2*node),w,b,X))**(1-Y))) - ((h(2*node+1,w,b,X)**Y) * ((1-h(2*node+1,w,b,X))**(1-Y))))) + ((h(2*node+1,w,b,X)**Y) * ((1-h(2*node+1,w,b,X))**(1-Y)))
  else:
    count = count + 1
    return (h(node,w,b,X) * (update_gradient(w,b,X,Y,n_layers,2*node,j,count) - update_gradient(w,b,X,Y,n_layers,2*node+1,j,count))) + update_gradient(w,b,X,Y,n_layers,2*node+1,j,count)

def calculate_gradient(p_ensemble, w, b, X, Y, n_layers):
    m = X.shape[0]  # Number of samples
    n_nodes = w.shape[0]  # Number of nodes
    w_gradient = np.zeros_like(w)  # Adjusting shape initialization
    b_gradient = np.zeros_like(b)  # Adjusting shape initialization
    for j in range(n_nodes):
        if j+1 >= 2**(n_layers-1):
            p_path = p_path_probability(n_layers, j+1, w, b, X)
            h_val = h(j+1, w, b, X)
            gradient_component = (p_path * (h_val**Y) * ((1-h_val)**(1-Y)) * (Y-h_val) / p_ensemble)
            w_gradient[j] = -np.mean(np.dot(X.T, gradient_component.T), axis=0)  # Take mean along samples to get shape (10,)
            b_gradient[j] = -np.mean(gradient_component)

        elif 2**(n_layers-2) <= j+1 < 2**(n_layers-1):
            p_path = p_path_probability(n_layers-1, j+1, w, b, X)
            h_val = h(j+1, w, b, X)
            h_val2 = h(2*(j+1), w, b, X)
            h_val3 = h(2*(j+1)+1, w, b, X)
            gradient_component = (p_path * h_val * (1-h_val) * (((h_val2**Y) * ((1-h_val2)**(1-Y))) - ((h_val3**Y) * ((1-h_val3)**(1-Y)))))/p_ensemble
            w_gradient[j] = -np.mean(np.dot(X.T, gradient_component.T), axis=0)  # Take mean along samples to get shape (10,)
            b_gradient[j] = -np.mean(gradient_component)

        elif n_layers > 2 and j+1 < 2**(n_layers-2):
            p_path = p_path_probability(np.floor(np.log2(j+1)+2)-1, j+1, w, b, X)
            h_val = h(j+1, w, b, X)
            gradient_component = (p_path * h_val * (1-h_val) * (update_gradient(w, b, X, Y, n_layers, 2*(j+1), j+1, 1) - update_gradient(w, b, X, Y, n_layers, 2*(j+1)+1, j+1, 1)))/p_ensemble
            w_gradient[j] = -np.mean(np.dot(X.T, gradient_component.T), axis=0)  # Take mean along samples to get shape (10,)
            b_gradient[j] = -np.mean(gradient_component)

    return w_gradient, b_gradient

def model_optimize(w, b, X, Y,n_layers=1):
    m = X.shape[0]
    p_ensemble = P(Y,X,n_layers,w,b)
    # Look for zeros in p_ensemble and replace it with 0.00000000001
    p_ensemble[p_ensemble == 0] = 0.00000000001
    cost = (-1/m)*(np.sum(np.log(p_ensemble)))

    dw,db = calculate_gradient(p_ensemble, w, b, X, Y, n_layers)
    grads = {"dw": dw, "db": db}

    return grads, cost

def model_predict(w, b, X, Y, learning_rate, no_iterations):
    costs = []
    for i in range(no_iterations):
        #
        grads, cost = model_optimize(w,b,X,Y.T)
        #
        dw = grads["dw"]
        db = grads["db"]

        #weight update
        w = w - (learning_rate * (dw))
        b = b - (learning_rate * db)
        #

        if (i % 100 == 0):
            costs.append(cost)
            #print("Cost after %i iteration is %f" %(i, cost))

    #final parameters
    coeff = {"w": w, "b": b}
    gradient = {"dw": dw, "db": db}

    return coeff, gradient, costs

def predict(final_pred, m):
    y_pred = np.zeros((1,m))
    for i in range(final_pred.shape[1]):
        if final_pred[0][i] >= 0.5:
            y_pred[0][i] = 1
    return y_pred

n_layers = 1
#Get number of features
n_features = X_tr_arr.shape[1]
print('Number of Features', n_features)
w, b= weightInitialization(n_features,n_layers)


#weightInitialization(n_features,n_layers)
print("Shape of w",w.shape)
print("Shape of b",b.shape)
print("Shape of X",X_tr_arr.shape)
print("Shape of Y",y_tr_arr.shape)

#Gradient Descent
coeff, gradient, costs = model_predict(w, b, X_tr_arr, y_tr_arr, learning_rate=0.0001,no_iterations=4500)
# w = fit(X_tr_arr, y_tr_arr, w, n_layers)
#Final prediction
w = coeff["w"]
b = coeff["b"]
print('Optimized weights', w)
print('Optimized intercept',b)
#
final_train_pred = P(1,X_tr_arr,n_layers,w,b) #ensemble_probability(w,b,X_tr_arr,n_layers)#sigmoid_activation(np.dot(w,X_tr_arr.T)+b)
final_test_pred = P(1,X_ts_arr,n_layers,w,b) #ensemble_probability(w,b,X_ts_arr,n_layers)#sigmoid_activation(np.dot(w,X_ts_arr.T)+b)
#
m_tr =  X_tr_arr.shape[0]
m_ts =  X_ts_arr.shape[0]
#
y_tr_pred = predict(final_train_pred.reshape(1,-1), m_tr)
print('Training Accuracy',accuracy_score(y_tr_pred.T, y_tr_arr))
#
y_ts_pred = predict(final_test_pred.reshape(1,-1), m_ts)
print('Test Accuracy',accuracy_score(y_ts_pred.T, y_ts_arr))

from sklearn.metrics import roc_curve, auc, recall_score, precision_score, accuracy_score

# # Calculate Accuracy
print('Training Accuracy:', accuracy_score(y_tr_pred.T, y_tr_arr))
print('Test Accuracy:', accuracy_score(y_ts_pred.T, y_ts_arr))

# ROC Curve and AUC for Test Set
fpr, tpr, thresholds = roc_curve(y_ts_arr, final_test_pred.T)
roc_auc = auc(fpr, tpr)
print('Test AUC:', roc_auc)

# Calculate Recall and Precision
recall = recall_score(y_ts_arr, y_ts_pred.T)
precision = precision_score(y_ts_arr, y_ts_pred.T)
print('Test Recall:', recall)
print('Test Precision:', precision)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

plt.plot(costs)
plt.ylabel('cost')
plt.xlabel('iterations (per hundreds)')
plt.title('Cost reduction over time')
plt.show()

n_layers = 2
#Get number of features
n_features = X_tr_arr.shape[1]
w, b= weightInitialization(n_features,n_layers)

#Gradient Descent
coeff, gradient, costs = model_predict(w, b, X_tr_arr, y_tr_arr, learning_rate=0.0001,no_iterations=4500)
# w = fit(X_tr_arr, y_tr_arr, w, n_layers)
#Final prediction
w = coeff["w"]
b = coeff["b"]
#
final_train_pred = P(1,X_tr_arr,n_layers,w,b) #ensemble_probability(w,b,X_tr_arr,n_layers)#sigmoid_activation(np.dot(w,X_tr_arr.T)+b)
final_test_pred = P(1,X_ts_arr,n_layers,w,b) #ensemble_probability(w,b,X_ts_arr,n_layers)#sigmoid_activation(np.dot(w,X_ts_arr.T)+b)
#
m_tr =  X_tr_arr.shape[0]
m_ts =  X_ts_arr.shape[0]
#
y_tr_pred = predict(final_train_pred.reshape(1,-1), m_tr)
print('Training Accuracy',accuracy_score(y_tr_pred.T, y_tr_arr))
#
y_ts_pred = predict(final_test_pred.reshape(1,-1), m_ts)
print('Test Accuracy',accuracy_score(y_ts_pred.T, y_ts_arr))

# # Calculate Accuracy
print('Training Accuracy:', accuracy_score(y_tr_pred.T, y_tr_arr))
print('Test Accuracy:', accuracy_score(y_ts_pred.T, y_ts_arr))

# ROC Curve and AUC for Test Set
fpr, tpr, thresholds = roc_curve(y_ts_arr, final_test_pred.T)
roc_auc = auc(fpr, tpr)
print('Test AUC:', roc_auc)

# Calculate Recall and Precision
recall = recall_score(y_ts_arr, y_ts_pred.T)
precision = precision_score(y_ts_arr, y_ts_pred.T)
print('Test Recall:', recall)
print('Test Precision:', precision)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

plt.plot(costs)
plt.ylabel('cost')
plt.xlabel('iterations (per hundreds)')
plt.title('Cost reduction over time')
plt.show()

n_layers = 3
#Get number of features
n_features = X_tr_arr.shape[1]
w, b= weightInitialization(n_features,n_layers)

#Gradient Descent
coeff, gradient, costs = model_predict(w, b, X_tr_arr, y_tr_arr, learning_rate=0.0001,no_iterations=4500)
# w = fit(X_tr_arr, y_tr_arr, w, n_layers)
#Final prediction
w = coeff["w"]
b = coeff["b"]
#
final_train_pred = P(1,X_tr_arr,n_layers,w,b) #ensemble_probability(w,b,X_tr_arr,n_layers)#sigmoid_activation(np.dot(w,X_tr_arr.T)+b)
final_test_pred = P(1,X_ts_arr,n_layers,w,b) #ensemble_probability(w,b,X_ts_arr,n_layers)#sigmoid_activation(np.dot(w,X_ts_arr.T)+b)
#
m_tr =  X_tr_arr.shape[0]
m_ts =  X_ts_arr.shape[0]
#
y_tr_pred = predict(final_train_pred.reshape(1,-1), m_tr)
print('Training Accuracy',accuracy_score(y_tr_pred.T, y_tr_arr))
#
y_ts_pred = predict(final_test_pred.reshape(1,-1), m_ts)
print('Test Accuracy',accuracy_score(y_ts_pred.T, y_ts_arr))

# # Calculate Accuracy
print('Training Accuracy:', accuracy_score(y_tr_pred.T, y_tr_arr))
print('Test Accuracy:', accuracy_score(y_ts_pred.T, y_ts_arr))

# ROC Curve and AUC for Test Set
fpr, tpr, thresholds = roc_curve(y_ts_arr, final_test_pred.T)
roc_auc = auc(fpr, tpr)
print('Test AUC:', roc_auc)

# Calculate Recall and Precision
recall = recall_score(y_ts_arr, y_ts_pred.T)
precision = precision_score(y_ts_arr, y_ts_pred.T)
print('Test Recall:', recall)
print('Test Precision:', precision)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

plt.plot(costs)
plt.ylabel('cost')
plt.xlabel('iterations (per hundreds)')
plt.title('Cost reduction over time')
plt.show()

n_layers = 4
#Get number of features
n_features = X_tr_arr.shape[1]
w, b= weightInitialization(n_features,n_layers)

#Gradient Descent
coeff, gradient, costs = model_predict(w, b, X_tr_arr, y_tr_arr, learning_rate=0.0001,no_iterations=4500)
# w = fit(X_tr_arr, y_tr_arr, w, n_layers)
#Final prediction
w = coeff["w"]
b = coeff["b"]
#
final_train_pred = P(1,X_tr_arr,n_layers,w,b) #ensemble_probability(w,b,X_tr_arr,n_layers)#sigmoid_activation(np.dot(w,X_tr_arr.T)+b)
final_test_pred = P(1,X_ts_arr,n_layers,w,b) #ensemble_probability(w,b,X_ts_arr,n_layers)#sigmoid_activation(np.dot(w,X_ts_arr.T)+b)
#
m_tr =  X_tr_arr.shape[0]
m_ts =  X_ts_arr.shape[0]
#
y_tr_pred = predict(final_train_pred.reshape(1,-1), m_tr)
print('Training Accuracy',accuracy_score(y_tr_pred.T, y_tr_arr))
#
y_ts_pred = predict(final_test_pred.reshape(1,-1), m_ts)
print('Test Accuracy',accuracy_score(y_ts_pred.T, y_ts_arr))

# # Calculate Accuracy
print('Training Accuracy:', accuracy_score(y_tr_pred.T, y_tr_arr))
print('Test Accuracy:', accuracy_score(y_ts_pred.T, y_ts_arr))

# ROC Curve and AUC for Test Set
fpr, tpr, thresholds = roc_curve(y_ts_arr, final_test_pred.T)
roc_auc = auc(fpr, tpr)
print('Test AUC:', roc_auc)

# Calculate Recall and Precision
recall = recall_score(y_ts_arr, y_ts_pred.T)
precision = precision_score(y_ts_arr, y_ts_pred.T)
print('Test Recall:', recall)
print('Test Precision:', precision)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

plt.plot(costs)
plt.ylabel('cost')
plt.xlabel('iterations (per hundreds)')
plt.title('Cost reduction over time')
plt.show()

n_layers = 5
#Get number of features
n_features = X_tr_arr.shape[1]
w, b= weightInitialization(n_features,n_layers)

#Gradient Descent
coeff, gradient, costs = model_predict(w, b, X_tr_arr, y_tr_arr, learning_rate=0.0001,no_iterations=4500)
# w = fit(X_tr_arr, y_tr_arr, w, n_layers)
#Final prediction
w = coeff["w"]
b = coeff["b"]
#
final_train_pred = P(1,X_tr_arr,n_layers,w,b) #ensemble_probability(w,b,X_tr_arr,n_layers)#sigmoid_activation(np.dot(w,X_tr_arr.T)+b)
final_test_pred = P(1,X_ts_arr,n_layers,w,b) #ensemble_probability(w,b,X_ts_arr,n_layers)#sigmoid_activation(np.dot(w,X_ts_arr.T)+b)
#
m_tr =  X_tr_arr.shape[0]
m_ts =  X_ts_arr.shape[0]
#
y_tr_pred = predict(final_train_pred.reshape(1,-1), m_tr)
print('Training Accuracy',accuracy_score(y_tr_pred.T, y_tr_arr))
#
y_ts_pred = predict(final_test_pred.reshape(1,-1), m_ts)
print('Test Accuracy',accuracy_score(y_ts_pred.T, y_ts_arr))

# # Calculate Accuracy
print('Training Accuracy:', accuracy_score(y_tr_pred.T, y_tr_arr))
print('Test Accuracy:', accuracy_score(y_ts_pred.T, y_ts_arr))

# ROC Curve and AUC for Test Set
fpr, tpr, thresholds = roc_curve(y_ts_arr, final_test_pred.T)
roc_auc = auc(fpr, tpr)
print('Test AUC:', roc_auc)

# Calculate Recall and Precision
recall = recall_score(y_ts_arr, y_ts_pred.T)
precision = precision_score(y_ts_arr, y_ts_pred.T)
print('Test Recall:', recall)
print('Test Precision:', precision)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

plt.plot(costs)
plt.ylabel('cost')
plt.xlabel('iterations (per hundreds)')
plt.title('Cost reduction over time')
plt.show()

n_layers = 6
#Get number of features
n_features = X_tr_arr.shape[1]
w, b= weightInitialization(n_features,n_layers)

#Gradient Descent
coeff, gradient, costs = model_predict(w, b, X_tr_arr, y_tr_arr, learning_rate=0.0001,no_iterations=4500)
# w = fit(X_tr_arr, y_tr_arr, w, n_layers)
#Final prediction
w = coeff["w"]
b = coeff["b"]
#
final_train_pred = P(1,X_tr_arr,n_layers,w,b) #ensemble_probability(w,b,X_tr_arr,n_layers)#sigmoid_activation(np.dot(w,X_tr_arr.T)+b)
final_test_pred = P(1,X_ts_arr,n_layers,w,b) #ensemble_probability(w,b,X_ts_arr,n_layers)#sigmoid_activation(np.dot(w,X_ts_arr.T)+b)
#
m_tr =  X_tr_arr.shape[0]
m_ts =  X_ts_arr.shape[0]
#
y_tr_pred = predict(final_train_pred.reshape(1,-1), m_tr)
print('Training Accuracy',accuracy_score(y_tr_pred.T, y_tr_arr))
#
y_ts_pred = predict(final_test_pred.reshape(1,-1), m_ts)
print('Test Accuracy',accuracy_score(y_ts_pred.T, y_ts_arr))

# # Calculate Accuracy
print('Training Accuracy:', accuracy_score(y_tr_pred.T, y_tr_arr))
print('Test Accuracy:', accuracy_score(y_ts_pred.T, y_ts_arr))

# ROC Curve and AUC for Test Set
fpr, tpr, thresholds = roc_curve(y_ts_arr, final_test_pred.T)
roc_auc = auc(fpr, tpr)
print('Test AUC:', roc_auc)

# Calculate Recall and Precision
recall = recall_score(y_ts_arr, y_ts_pred.T)
precision = precision_score(y_ts_arr, y_ts_pred.T)
print('Test Recall:', recall)
print('Test Precision:', precision)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

plt.plot(costs)
plt.ylabel('cost')
plt.xlabel('iterations (per hundreds)')
plt.title('Cost reduction over time')
plt.show()

from sklearn.ensemble import RandomForestClassifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train, y_train)
y_pred_rf = rf_classifier.predict(X_test)
print('Test Accuracy:', accuracy_score(y_test, y_pred_rf))

fpr, tpr, thresholds = roc_curve(y_test, y_pred_rf)
roc_auc = auc(fpr, tpr)
print('Test AUC:', roc_auc)

recall = recall_score(y_test, y_pred_rf)
precision = precision_score(y_test, y_pred_rf)
print('Test Recall:', recall)
print('Test Precision:', precision)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

from sklearn.ensemble import GradientBoostingClassifier
gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gbc.fit(X_train, y_train)
y_pred_gbc = gbc.predict(X_test)
print('Test Accuracy:', accuracy_score(y_test, y_pred_gbc))

fpr, tpr, thresholds = roc_curve(y_test, y_pred_gbc)
roc_auc = auc(fpr, tpr)
print('Test AUC:', roc_auc)

recall = recall_score(y_test, y_pred_gbc)
precision = precision_score(y_test, y_pred_gbc)
print('Test Recall:', recall)
print('Test Precision:', precision)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

!pip install ucimlrepo
from ucimlrepo import fetch_ucirepo
covertype = fetch_ucirepo(id=31)

X = covertype.data.features
y = covertype.data.targets

cover_X = pd.DataFrame(X)
cover_y = pd.DataFrame(y)
cover_df = pd.concat([cover_X, cover_y], axis=1)
print(cover_df.head())


import random
subset = cover_df.sample(frac=0.008, random_state=42)

subset['Cover_Type'] = subset['Cover_Type'].apply(lambda x: 1 if x != 2 else 2)
subset['Cover_Type'] = subset['Cover_Type'].apply(lambda x: 0 if x == 2 else 1)
cover_df["Cover_Type"].value_counts()


X2 = subset.drop('Cover_Type', axis=1)
y2 = subset['Cover_Type']

subset['Cover_Type'].value_counts().plot.bar()

scaler = StandardScaler()
X2_scaled = scaler.fit_transform(X2)
X2_train, X2_test, y2_train, y2_test = train_test_split(X2_scaled, y2, test_size=0.2, random_state=42)
smote = SMOTE(random_state=42)
X2_train, y2_train = smote.fit_resample(X2_train, y2_train)
y2_train.value_counts().plot.bar()

X2_tr_arr = X2_train
X2_ts_arr = X2_test
y2_tr_arr = np.array(y2_train)
y2_ts_arr = np.array(y2_test)

print('Input Shape', (X2_tr_arr.shape))
print('Output Shape', X2_test.shape)

n_features2 = X2_tr_arr.shape[1]
print('Number of Features', n_features2)

n_layers = 1
#Get number of features
n_features2 = X2_tr_arr.shape[1]
print('Number of Features', n_features2)
w, b= weightInitialization(n_features2,n_layers)


#weightInitialization(n_features,n_layers)
print("Shape of w",w.shape)
print("Shape of b",b.shape)
print("Shape of X",X2_tr_arr.shape)
print("Shape of Y",y2_tr_arr.shape)

#Gradient Descent
coeff, gradient, costs = model_predict(w, b, X2_tr_arr, y2_tr_arr, learning_rate=0.0001,no_iterations=4500)
# w = fit(X_tr_arr, y_tr_arr, w, n_layers)
#Final prediction
w = coeff["w"]
b = coeff["b"]
print('Optimized weights', w)
print('Optimized intercept',b)
#
final_train_pred = P(1,X2_tr_arr,n_layers,w,b) #ensemble_probability(w,b,X_tr_arr,n_layers)#sigmoid_activation(np.dot(w,X_tr_arr.T)+b)
final_test_pred = P(1,X2_ts_arr,n_layers,w,b) #ensemble_probability(w,b,X_ts_arr,n_layers)#sigmoid_activation(np.dot(w,X_ts_arr.T)+b)
#
m_tr =  X2_tr_arr.shape[0]
m_ts =  X2_ts_arr.shape[0]
#
y_tr_pred = predict(final_train_pred.reshape(1,-1), m_tr)
print('Training Accuracy',accuracy_score(y_tr_pred.T, y2_tr_arr))
#
y_ts_pred = predict(final_test_pred.reshape(1,-1), m_ts)
print('Test Accuracy',accuracy_score(y_ts_pred.T, y2_ts_arr))

# # Calculate Accuracy
print('Training Accuracy:', accuracy_score(y_tr_pred.T, y2_tr_arr))
print('Test Accuracy:', accuracy_score(y_ts_pred.T, y2_ts_arr))

# ROC Curve and AUC for Test Set
fpr, tpr, thresholds = roc_curve(y2_ts_arr, final_test_pred.T)
roc_auc = auc(fpr, tpr)
print('Test AUC:', roc_auc)

# Calculate Recall and Precision
recall = recall_score(y2_ts_arr, y_ts_pred.T)
precision = precision_score(y2_ts_arr, y_ts_pred.T)
print('Test Recall:', recall)
print('Test Precision:', precision)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

plt.plot(costs)
plt.ylabel('cost')
plt.xlabel('iterations (per hundreds)')
plt.title('Cost reduction over time')
plt.show()

n_layers = 2
#Get number of features
n_features2 = X2_tr_arr.shape[1]
w, b= weightInitialization(n_features2,n_layers)

#Gradient Descent
coeff, gradient, costs = model_predict(w, b, X2_tr_arr, y2_tr_arr, learning_rate=0.0001,no_iterations=4500)
# w = fit(X_tr_arr, y_tr_arr, w, n_layers)
#Final prediction
w = coeff["w"]
b = coeff["b"]
#
final_train_pred = P(1,X2_tr_arr,n_layers,w,b) #ensemble_probability(w,b,X_tr_arr,n_layers)#sigmoid_activation(np.dot(w,X_tr_arr.T)+b)
final_test_pred = P(1,X2_ts_arr,n_layers,w,b) #ensemble_probability(w,b,X_ts_arr,n_layers)#sigmoid_activation(np.dot(w,X_ts_arr.T)+b)
#
m_tr =  X2_tr_arr.shape[0]
m_ts =  X2_ts_arr.shape[0]
#
y_tr_pred = predict(final_train_pred.reshape(1,-1), m_tr)
print('Training Accuracy',accuracy_score(y_tr_pred.T, y2_tr_arr))
#
y_ts_pred = predict(final_test_pred.reshape(1,-1), m_ts)
print('Test Accuracy',accuracy_score(y_ts_pred.T, y2_ts_arr))

# # Calculate Accuracy
print('Training Accuracy:', accuracy_score(y_tr_pred.T, y2_tr_arr))
print('Test Accuracy:', accuracy_score(y_ts_pred.T, y2_ts_arr))

# ROC Curve and AUC for Test Set
fpr, tpr, thresholds = roc_curve(y2_ts_arr, final_test_pred.T)
roc_auc = auc(fpr, tpr)
print('Test AUC:', roc_auc)

# Calculate Recall and Precision
recall = recall_score(y2_ts_arr, y_ts_pred.T)
precision = precision_score(y2_ts_arr, y_ts_pred.T)
print('Test Recall:', recall)
print('Test Precision:', precision)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

plt.plot(costs)
plt.ylabel('cost')
plt.xlabel('iterations (per hundreds)')
plt.title('Cost reduction over time')
plt.show()

n_layers = 3
#Get number of features
n_features2 = X2_tr_arr.shape[1]
w, b= weightInitialization(n_features2,n_layers)

#Gradient Descent
coeff, gradient, costs = model_predict(w, b, X2_tr_arr, y2_tr_arr, learning_rate=0.0001,no_iterations=4500)
# w = fit(X_tr_arr, y_tr_arr, w, n_layers)
#Final prediction
w = coeff["w"]
b = coeff["b"]
#
final_train_pred = P(1,X2_tr_arr,n_layers,w,b) #ensemble_probability(w,b,X_tr_arr,n_layers)#sigmoid_activation(np.dot(w,X_tr_arr.T)+b)
final_test_pred = P(1,X2_ts_arr,n_layers,w,b) #ensemble_probability(w,b,X_ts_arr,n_layers)#sigmoid_activation(np.dot(w,X_ts_arr.T)+b)
#
m_tr =  X2_tr_arr.shape[0]
m_ts =  X2_ts_arr.shape[0]
#
y_tr_pred = predict(final_train_pred.reshape(1,-1), m_tr)
print('Training Accuracy',accuracy_score(y_tr_pred.T, y2_tr_arr))
#
y_ts_pred = predict(final_test_pred.reshape(1,-1), m_ts)
print('Test Accuracy',accuracy_score(y_ts_pred.T, y2_ts_arr))

# # Calculate Accuracy
print('Training Accuracy:', accuracy_score(y_tr_pred.T, y2_tr_arr))
print('Test Accuracy:', accuracy_score(y_ts_pred.T, y2_ts_arr))

# ROC Curve and AUC for Test Set
fpr, tpr, thresholds = roc_curve(y2_ts_arr, final_test_pred.T)
roc_auc = auc(fpr, tpr)
print('Test AUC:', roc_auc)

# Calculate Recall and Precision
recall = recall_score(y2_ts_arr, y_ts_pred.T)
precision = precision_score(y2_ts_arr, y_ts_pred.T)
print('Test Recall:', recall)
print('Test Precision:', precision)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

plt.plot(costs)
plt.ylabel('cost')
plt.xlabel('iterations (per hundreds)')
plt.title('Cost reduction over time')
plt.show()

n_layers = 4
#Get number of features
n_features2 = X2_tr_arr.shape[1]
w, b= weightInitialization(n_features2,n_layers)

#Gradient Descent
coeff, gradient, costs = model_predict(w, b, X2_tr_arr, y2_tr_arr, learning_rate=0.0001,no_iterations=4500)
# w = fit(X_tr_arr, y_tr_arr, w, n_layers)
#Final prediction
w = coeff["w"]
b = coeff["b"]
#
final_train_pred = P(1,X2_tr_arr,n_layers,w,b) #ensemble_probability(w,b,X_tr_arr,n_layers)#sigmoid_activation(np.dot(w,X_tr_arr.T)+b)
final_test_pred = P(1,X2_ts_arr,n_layers,w,b) #ensemble_probability(w,b,X_ts_arr,n_layers)#sigmoid_activation(np.dot(w,X_ts_arr.T)+b)
#
m_tr =  X2_tr_arr.shape[0]
m_ts =  X2_ts_arr.shape[0]
#
y_tr_pred = predict(final_train_pred.reshape(1,-1), m_tr)
print('Training Accuracy',accuracy_score(y_tr_pred.T, y2_tr_arr))
#
y_ts_pred = predict(final_test_pred.reshape(1,-1), m_ts)
print('Test Accuracy',accuracy_score(y_ts_pred.T, y2_ts_arr))

# # Calculate Accuracy
print('Training Accuracy:', accuracy_score(y_tr_pred.T, y2_tr_arr))
print('Test Accuracy:', accuracy_score(y_ts_pred.T, y2_ts_arr))

# ROC Curve and AUC for Test Set
fpr, tpr, thresholds = roc_curve(y2_ts_arr, final_test_pred.T)
roc_auc = auc(fpr, tpr)
print('Test AUC:', roc_auc)

# Calculate Recall and Precision
recall = recall_score(y2_ts_arr, y_ts_pred.T)
precision = precision_score(y2_ts_arr, y_ts_pred.T)
print('Test Recall:', recall)
print('Test Precision:', precision)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

plt.plot(costs)
plt.ylabel('cost')
plt.xlabel('iterations (per hundreds)')
plt.title('Cost reduction over time')
plt.show()

n_layers = 5
#Get number of features
n_features2 = X2_tr_arr.shape[1]
w, b= weightInitialization(n_features2,n_layers)

#Gradient Descent
coeff, gradient, costs = model_predict(w, b, X2_tr_arr, y2_tr_arr, learning_rate=0.0001,no_iterations=4500)
# w = fit(X_tr_arr, y_tr_arr, w, n_layers)
#Final prediction
w = coeff["w"]
b = coeff["b"]
#
final_train_pred = P(1,X2_tr_arr,n_layers,w,b) #ensemble_probability(w,b,X_tr_arr,n_layers)#sigmoid_activation(np.dot(w,X_tr_arr.T)+b)
final_test_pred = P(1,X2_ts_arr,n_layers,w,b) #ensemble_probability(w,b,X_ts_arr,n_layers)#sigmoid_activation(np.dot(w,X_ts_arr.T)+b)
#
m_tr =  X2_tr_arr.shape[0]
m_ts =  X2_ts_arr.shape[0]
#
y_tr_pred = predict(final_train_pred.reshape(1,-1), m_tr)
print('Training Accuracy',accuracy_score(y_tr_pred.T, y2_tr_arr))
#
y_ts_pred = predict(final_test_pred.reshape(1,-1), m_ts)
print('Test Accuracy',accuracy_score(y_ts_pred.T, y2_ts_arr))

# # Calculate Accuracy
print('Training Accuracy:', accuracy_score(y_tr_pred.T, y2_tr_arr))
print('Test Accuracy:', accuracy_score(y_ts_pred.T, y2_ts_arr))

# ROC Curve and AUC for Test Set
fpr, tpr, thresholds = roc_curve(y2_ts_arr, final_test_pred.T)
roc_auc = auc(fpr, tpr)
print('Test AUC:', roc_auc)

# Calculate Recall and Precision
recall = recall_score(y2_ts_arr, y_ts_pred.T)
precision = precision_score(y2_ts_arr, y_ts_pred.T)
print('Test Recall:', recall)
print('Test Precision:', precision)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

plt.plot(costs)
plt.ylabel('cost')
plt.xlabel('iterations (per hundreds)')
plt.title('Cost reduction over time')
plt.show()

n_layers = 6
#Get number of features
n_features2 = X2_tr_arr.shape[1]
w, b= weightInitialization(n_features2,n_layers)

#Gradient Descent
coeff, gradient, costs = model_predict(w, b, X2_tr_arr, y2_tr_arr, learning_rate=0.0001,no_iterations=4500)
# w = fit(X_tr_arr, y_tr_arr, w, n_layers)
#Final prediction
w = coeff["w"]
b = coeff["b"]
#
final_train_pred = P(1,X2_tr_arr,n_layers,w,b) #ensemble_probability(w,b,X_tr_arr,n_layers)#sigmoid_activation(np.dot(w,X_tr_arr.T)+b)
final_test_pred = P(1,X2_ts_arr,n_layers,w,b) #ensemble_probability(w,b,X_ts_arr,n_layers)#sigmoid_activation(np.dot(w,X_ts_arr.T)+b)
#
m_tr =  X2_tr_arr.shape[0]
m_ts =  X2_ts_arr.shape[0]
#
y_tr_pred = predict(final_train_pred.reshape(1,-1), m_tr)
print('Training Accuracy',accuracy_score(y_tr_pred.T, y2_tr_arr))
#
y_ts_pred = predict(final_test_pred.reshape(1,-1), m_ts)
print('Test Accuracy',accuracy_score(y_ts_pred.T, y2_ts_arr))

# # Calculate Accuracy
print('Training Accuracy:', accuracy_score(y_tr_pred.T, y2_tr_arr))
print('Test Accuracy:', accuracy_score(y_ts_pred.T, y2_ts_arr))

# ROC Curve and AUC for Test Set
fpr, tpr, thresholds = roc_curve(y2_ts_arr, final_test_pred.T)
roc_auc = auc(fpr, tpr)
print('Test AUC:', roc_auc)

# Calculate Recall and Precision
recall = recall_score(y2_ts_arr, y_ts_pred.T)
precision = precision_score(y2_ts_arr, y_ts_pred.T)
print('Test Recall:', recall)
print('Test Precision:', precision)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

plt.plot(costs)
plt.ylabel('cost')
plt.xlabel('iterations (per hundreds)')
plt.title('Cost reduction over time')
plt.show()

rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X2_train, y2_train)
y2_pred_rf = rf_classifier.predict(X2_test)
print('Test Accuracy:', accuracy_score(y2_test, y2_pred_rf))

fpr, tpr, thresholds = roc_curve(y2_test, y2_pred_rf)
roc_auc = auc(fpr, tpr)
print('Test AUC:', roc_auc)

recall = recall_score(y2_test, y2_pred_rf)
precision = precision_score(y2_test, y2_pred_rf)
print('Test Recall:', recall)
print('Test Precision:', precision)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

gbc2 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gbc2.fit(X2_train, y2_train)
y2_pred_gbc = gbc2.predict(X2_test)
print('Test Accuracy:', accuracy_score(y2_test, y2_pred_gbc))

fpr, tpr, thresholds = roc_curve(y2_test, y2_pred_gbc)
roc_auc = auc(fpr, tpr)
print('Test AUC:', roc_auc)

recall = recall_score(y2_test, y2_pred_gbc)
precision = precision_score(y2_test, y2_pred_gbc)
print('Test Recall:', recall)
print('Test Precision:', precision)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

from sklearn.preprocessing import LabelEncoder
df3 = pd.read_csv('/content/drive/MyDrive/Titanic-Dataset.csv')

label_encoder = LabelEncoder()

df3['Cabin'] = label_encoder.fit_transform(df3['Cabin'])

df3['Ticket'] = label_encoder.fit_transform(df3['Ticket'])

df3['Embarked'] = label_encoder.fit_transform(df3['Embarked'])

df3['Sex'] = df3['Sex'].apply(lambda x: 0 if x == "male" else 1)

df3 = df3.dropna()
df3.head()

X3 = df3.drop(['Survived', "PassengerId", "Name"], axis=1)
y3 = df3['Survived']
df3['Survived'].value_counts().plot.bar()

scaler = StandardScaler()
X3 = scaler.fit_transform(X3)
X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=0.2, random_state=42)

X3_tr_arr = X3_train
X3_ts_arr = X3_test
y3_tr_arr = np.array(y3_train)
y3_ts_arr = np.array(y3_test)

print('Input Shape', (X3_tr_arr.shape))
print('Output Shape', X3_test.shape)

n_features3 = X3_tr_arr.shape[1]
print('Number of Features', n_features3)

n_layers = 1
#Get number of features
n_features3 = X3_tr_arr.shape[1]
print('Number of Features', n_features3)
w, b= weightInitialization(n_features3,n_layers)


#weightInitialization(n_features,n_layers)
print("Shape of w",w.shape)
print("Shape of b",b.shape)
print("Shape of X",X3_tr_arr.shape)
print("Shape of Y",y3_tr_arr.shape)

#Gradient Descent
coeff, gradient, costs = model_predict(w, b, X3_tr_arr, y3_tr_arr, learning_rate=0.0001,no_iterations=4500)
# w = fit(X_tr_arr, y_tr_arr, w, n_layers)
#Final prediction
w = coeff["w"]
b = coeff["b"]
print('Optimized weights', w)
print('Optimized intercept',b)
#
final_train_pred = P(1,X3_tr_arr,n_layers,w,b) #ensemble_probability(w,b,X_tr_arr,n_layers)#sigmoid_activation(np.dot(w,X_tr_arr.T)+b)
final_test_pred = P(1,X3_ts_arr,n_layers,w,b) #ensemble_probability(w,b,X_ts_arr,n_layers)#sigmoid_activation(np.dot(w,X_ts_arr.T)+b)
#
m_tr =  X3_tr_arr.shape[0]
m_ts =  X3_ts_arr.shape[0]
#
y_tr_pred = predict(final_train_pred.reshape(1,-1), m_tr)
print('Training Accuracy',accuracy_score(y_tr_pred.T, y3_tr_arr))
#
y_ts_pred = predict(final_test_pred.reshape(1,-1), m_ts)
print('Test Accuracy',accuracy_score(y_ts_pred.T, y3_ts_arr))

# # Calculate Accuracy
print('Training Accuracy:', accuracy_score(y_tr_pred.T, y3_tr_arr))
print('Test Accuracy:', accuracy_score(y_ts_pred.T, y3_ts_arr))

# ROC Curve and AUC for Test Set
fpr, tpr, thresholds = roc_curve(y3_ts_arr, final_test_pred.T)
roc_auc = auc(fpr, tpr)
print('Test AUC:', roc_auc)

# Calculate Recall and Precision
recall = recall_score(y3_ts_arr, y_ts_pred.T)
precision = precision_score(y3_ts_arr, y_ts_pred.T)
print('Test Recall:', recall)
print('Test Precision:', precision)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

plt.plot(costs)
plt.ylabel('cost')
plt.xlabel('iterations (per hundreds)')
plt.title('Cost reduction over time')
plt.show()

n_layers = 2

n_features3 = X3_tr_arr.shape[1]
w, b= weightInitialization(n_features3,n_layers)

#Gradient Descent
coeff, gradient, costs = model_predict(w, b, X3_tr_arr, y3_tr_arr, learning_rate=0.0001,no_iterations=4500)
# w = fit(X_tr_arr, y_tr_arr, w, n_layers)
#Final prediction
w = coeff["w"]
b = coeff["b"]
#
final_train_pred = P(1,X3_tr_arr,n_layers,w,b) #ensemble_probability(w,b,X_tr_arr,n_layers)#sigmoid_activation(np.dot(w,X_tr_arr.T)+b)
final_test_pred = P(1,X3_ts_arr,n_layers,w,b) #ensemble_probability(w,b,X_ts_arr,n_layers)#sigmoid_activation(np.dot(w,X_ts_arr.T)+b)
#
m_tr =  X3_tr_arr.shape[0]
m_ts =  X3_ts_arr.shape[0]
#
y_tr_pred = predict(final_train_pred.reshape(1,-1), m_tr)
print('Training Accuracy',accuracy_score(y_tr_pred.T, y3_tr_arr))
#
y_ts_pred = predict(final_test_pred.reshape(1,-1), m_ts)
print('Test Accuracy',accuracy_score(y_ts_pred.T, y3_ts_arr))

# # Calculate Accuracy
print('Training Accuracy:', accuracy_score(y_tr_pred.T, y3_tr_arr))
print('Test Accuracy:', accuracy_score(y_ts_pred.T, y3_ts_arr))

# ROC Curve and AUC for Test Set
fpr, tpr, thresholds = roc_curve(y3_ts_arr, final_test_pred.T)
roc_auc = auc(fpr, tpr)
print('Test AUC:', roc_auc)

# Calculate Recall and Precision
recall = recall_score(y3_ts_arr, y_ts_pred.T)
precision = precision_score(y3_ts_arr, y_ts_pred.T)
print('Test Recall:', recall)
print('Test Precision:', precision)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

plt.plot(costs)
plt.ylabel('cost')
plt.xlabel('iterations (per hundreds)')
plt.title('Cost reduction over time')
plt.show()

n_layers = 3

n_features3 = X3_tr_arr.shape[1]
w, b= weightInitialization(n_features3,n_layers)

#Gradient Descent
coeff, gradient, costs = model_predict(w, b, X3_tr_arr, y3_tr_arr, learning_rate=0.0001,no_iterations=4500)
# w = fit(X_tr_arr, y_tr_arr, w, n_layers)
#Final prediction
w = coeff["w"]
b = coeff["b"]
#
final_train_pred = P(1,X3_tr_arr,n_layers,w,b) #ensemble_probability(w,b,X_tr_arr,n_layers)#sigmoid_activation(np.dot(w,X_tr_arr.T)+b)
final_test_pred = P(1,X3_ts_arr,n_layers,w,b) #ensemble_probability(w,b,X_ts_arr,n_layers)#sigmoid_activation(np.dot(w,X_ts_arr.T)+b)
#
m_tr =  X3_tr_arr.shape[0]
m_ts =  X3_ts_arr.shape[0]
#
y_tr_pred = predict(final_train_pred.reshape(1,-1), m_tr)
print('Training Accuracy',accuracy_score(y_tr_pred.T, y3_tr_arr))
#
y_ts_pred = predict(final_test_pred.reshape(1,-1), m_ts)
print('Test Accuracy',accuracy_score(y_ts_pred.T, y3_ts_arr))

# # Calculate Accuracy
print('Training Accuracy:', accuracy_score(y_tr_pred.T, y3_tr_arr))
print('Test Accuracy:', accuracy_score(y_ts_pred.T, y3_ts_arr))

# ROC Curve and AUC for Test Set
fpr, tpr, thresholds = roc_curve(y3_ts_arr, final_test_pred.T)
roc_auc = auc(fpr, tpr)
print('Test AUC:', roc_auc)

# Calculate Recall and Precision
recall = recall_score(y3_ts_arr, y_ts_pred.T)
precision = precision_score(y3_ts_arr, y_ts_pred.T)
print('Test Recall:', recall)
print('Test Precision:', precision)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

plt.plot(costs)
plt.ylabel('cost')
plt.xlabel('iterations (per hundreds)')
plt.title('Cost reduction over time')
plt.show()

n_layers = 4

n_features3 = X3_tr_arr.shape[1]
w, b= weightInitialization(n_features3,n_layers)

#Gradient Descent
coeff, gradient, costs = model_predict(w, b, X3_tr_arr, y3_tr_arr, learning_rate=0.0001,no_iterations=4500)
# w = fit(X_tr_arr, y_tr_arr, w, n_layers)
#Final prediction
w = coeff["w"]
b = coeff["b"]
#
final_train_pred = P(1,X3_tr_arr,n_layers,w,b) #ensemble_probability(w,b,X_tr_arr,n_layers)#sigmoid_activation(np.dot(w,X_tr_arr.T)+b)
final_test_pred = P(1,X3_ts_arr,n_layers,w,b) #ensemble_probability(w,b,X_ts_arr,n_layers)#sigmoid_activation(np.dot(w,X_ts_arr.T)+b)
#
m_tr =  X3_tr_arr.shape[0]
m_ts =  X3_ts_arr.shape[0]
#
y_tr_pred = predict(final_train_pred.reshape(1,-1), m_tr)
print('Training Accuracy',accuracy_score(y_tr_pred.T, y3_tr_arr))
#
y_ts_pred = predict(final_test_pred.reshape(1,-1), m_ts)
print('Test Accuracy',accuracy_score(y_ts_pred.T, y3_ts_arr))

# # Calculate Accuracy
print('Training Accuracy:', accuracy_score(y_tr_pred.T, y3_tr_arr))
print('Test Accuracy:', accuracy_score(y_ts_pred.T, y3_ts_arr))

# ROC Curve and AUC for Test Set
fpr, tpr, thresholds = roc_curve(y3_ts_arr, final_test_pred.T)
roc_auc = auc(fpr, tpr)
print('Test AUC:', roc_auc)

# Calculate Recall and Precision
recall = recall_score(y3_ts_arr, y_ts_pred.T)
precision = precision_score(y3_ts_arr, y_ts_pred.T)
print('Test Recall:', recall)
print('Test Precision:', precision)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

plt.plot(costs)
plt.ylabel('cost')
plt.xlabel('iterations (per hundreds)')
plt.title('Cost reduction over time')
plt.show()

n_layers = 5

n_features3 = X3_tr_arr.shape[1]
w, b= weightInitialization(n_features3,n_layers)

#Gradient Descent
coeff, gradient, costs = model_predict(w, b, X3_tr_arr, y3_tr_arr, learning_rate=0.0001,no_iterations=4500)
# w = fit(X_tr_arr, y_tr_arr, w, n_layers)
#Final prediction
w = coeff["w"]
b = coeff["b"]
#
final_train_pred = P(1,X3_tr_arr,n_layers,w,b) #ensemble_probability(w,b,X_tr_arr,n_layers)#sigmoid_activation(np.dot(w,X_tr_arr.T)+b)
final_test_pred = P(1,X3_ts_arr,n_layers,w,b) #ensemble_probability(w,b,X_ts_arr,n_layers)#sigmoid_activation(np.dot(w,X_ts_arr.T)+b)
#
m_tr =  X3_tr_arr.shape[0]
m_ts =  X3_ts_arr.shape[0]
#
y_tr_pred = predict(final_train_pred.reshape(1,-1), m_tr)
print('Training Accuracy',accuracy_score(y_tr_pred.T, y3_tr_arr))
#
y_ts_pred = predict(final_test_pred.reshape(1,-1), m_ts)
print('Test Accuracy',accuracy_score(y_ts_pred.T, y3_ts_arr))

# # Calculate Accuracy
print('Training Accuracy:', accuracy_score(y_tr_pred.T, y3_tr_arr))
print('Test Accuracy:', accuracy_score(y_ts_pred.T, y3_ts_arr))

# ROC Curve and AUC for Test Set
fpr, tpr, thresholds = roc_curve(y3_ts_arr, final_test_pred.T)
roc_auc = auc(fpr, tpr)
print('Test AUC:', roc_auc)

# Calculate Recall and Precision
recall = recall_score(y3_ts_arr, y_ts_pred.T)
precision = precision_score(y3_ts_arr, y_ts_pred.T)
print('Test Recall:', recall)
print('Test Precision:', precision)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

plt.plot(costs)
plt.ylabel('cost')
plt.xlabel('iterations (per hundreds)')
plt.title('Cost reduction over time')
plt.show()

n_layers = 6

n_features3 = X3_tr_arr.shape[1]
w, b= weightInitialization(n_features3,n_layers)

#Gradient Descent
coeff, gradient, costs = model_predict(w, b, X3_tr_arr, y3_tr_arr, learning_rate=0.0001,no_iterations=4500)
# w = fit(X_tr_arr, y_tr_arr, w, n_layers)
#Final prediction
w = coeff["w"]
b = coeff["b"]
#
final_train_pred = P(1,X3_tr_arr,n_layers,w,b) #ensemble_probability(w,b,X_tr_arr,n_layers)#sigmoid_activation(np.dot(w,X_tr_arr.T)+b)
final_test_pred = P(1,X3_ts_arr,n_layers,w,b) #ensemble_probability(w,b,X_ts_arr,n_layers)#sigmoid_activation(np.dot(w,X_ts_arr.T)+b)
#
m_tr =  X3_tr_arr.shape[0]
m_ts =  X3_ts_arr.shape[0]
#
y_tr_pred = predict(final_train_pred.reshape(1,-1), m_tr)
print('Training Accuracy',accuracy_score(y_tr_pred.T, y3_tr_arr))
#
y_ts_pred = predict(final_test_pred.reshape(1,-1), m_ts)
print('Test Accuracy',accuracy_score(y_ts_pred.T, y3_ts_arr))

# # Calculate Accuracy
print('Training Accuracy:', accuracy_score(y_tr_pred.T, y3_tr_arr))
print('Test Accuracy:', accuracy_score(y_ts_pred.T, y3_ts_arr))

# ROC Curve and AUC for Test Set
fpr, tpr, thresholds = roc_curve(y3_ts_arr, final_test_pred.T)
roc_auc = auc(fpr, tpr)
print('Test AUC:', roc_auc)

# Calculate Recall and Precision
recall = recall_score(y3_ts_arr, y_ts_pred.T)
precision = precision_score(y3_ts_arr, y_ts_pred.T)
print('Test Recall:', recall)
print('Test Precision:', precision)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

plt.plot(costs)
plt.ylabel('cost')
plt.xlabel('iterations (per hundreds)')
plt.title('Cost reduction over time')
plt.show()

from sklearn.ensemble import RandomForestClassifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X3_train, y3_train)
y_pred_rf = rf_classifier.predict(X3_test)
print('Test Accuracy:', accuracy_score(y3_test, y_pred_rf))

fpr, tpr, thresholds = roc_curve(y3_test, y_pred_rf)
roc_auc = auc(fpr, tpr)
print('Test AUC:', roc_auc)

recall = recall_score(y3_test, y_pred_rf)
precision = precision_score(y3_test, y_pred_rf)
print('Test Recall:', recall)
print('Test Precision:', precision)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

from sklearn.ensemble import GradientBoostingClassifier
gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gbc.fit(X3_train, y3_train)
y_pred_gbc = gbc.predict(X3_test)
print('Test Accuracy:', accuracy_score(y3_test, y_pred_gbc))

fpr, tpr, thresholds = roc_curve(y3_test, y_pred_gbc)
roc_auc = auc(fpr, tpr)
print('Test AUC:', roc_auc)

recall = recall_score(y3_test, y_pred_gbc)
precision = precision_score(y3_test, y_pred_gbc)
print('Test Recall:', recall)
print('Test Precision:', precision)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()